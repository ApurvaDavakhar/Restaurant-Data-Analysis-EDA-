# -*- coding: utf-8 -*-
"""Restaurant_Data_Analysis_Level_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/Apaulgithub/Restaurant_Data_Analysis_and_Prediction/blob/main/Level%201/Restaurant_Data_Analysis_Level_1.ipynb

# **Project Name**    - Restaurant_Data_Analysis_Level_1

##### **Project Type**    - EDA
##### **Industry**    - Cognifyz Technologies
##### **Contribution**    - Individual
##### **Member Name -** Apurva Davakhar
##### **Level -** 1

# **Project Summary -**

This level 1 tasks of the project focuses on analyzing restaurant data, exploring insights from a dataset consisting of different restaurants name, city, address, locality, rating, price range etc.

The Level 1 tasks undertaken during the Cognifyz Data Science Internship, focusing on data exploration and preprocessing, descriptive analysis, and geospatial analysis of restaurant data. It will be interesting to explore what all other insights can be obtained from the same dataset.

**Level 1 Tasks:**

**Task 1: Data Exploration and Preprocessing**

- Explored the restaurant dataset, determining its dimensions.
- Managed missing values across columns, ensuring data integrity.
- Executed data type conversions as needed.
- Analyzed the distribution of the target variable, "Aggregate rating," and addressed class imbalances.

**Task 2: Descriptive Analysis**

- Calculated fundamental statistical measures (e.g., mean, median, standard deviation) for numerical columns.
- Investigated the distribution of categorical variables like "Country Code," "City," and "Cuisines."
- Identified the top cuisines and cities with the highest restaurant counts.


**Task 3: Geospatial Analysis**

- Visualized restaurant locations on maps using latitude and longitude data.
- Conducted an analysis of restaurant distribution across different cities and countries.
- Explored potential correlations between restaurant locations and ratings.

So, this notebook consist of all the Level 1 tasks which i completed during the Cognifyz Data Science Internship. The tasks encompass data exploration, data preprocessing, statistical analysis, and geospatial insights within the restaurant industry, demonstrating a foundational understanding of data science principles.

# **GitHub Link -**

**GitHub Link:**
https://github.com/Apaulgithub/Restaurant_Data_Analysis_and_Prediction/tree/main/Level%201

# **Problem Statement**

The Level 1 of the Cognifyz Data Science Internship, focuses on the exploration and analysis of a restaurant dataset. The level comprises three key tasks: Data Exploration and Preprocessing, Descriptive Analysis, and Geospatial Analysis.

**Project Objectives:**

- Gain proficiency in data exploration and preprocessing.
- Perform descriptive analysis to understand dataset characteristics.
- Apply geospatial analysis techniques to uncover location-based insights.
- Develop foundational data science skills for the restaurant industry.

**Key Tasks in Level 1:**

**Task 1: Data Exploration and Preprocessing**

- Explore the dataset to understand its structure, including the number of rows and columns.
- Address missing values in each column, ensuring data integrity.
- Perform data type conversions as necessary.
- Analyze the distribution of the target variable ("Aggregate rating") and identify potential class imbalances.

**Task 2: Descriptive Analysis**

- Calculate essential statistical measures (e.g., mean, median, standard deviation) for numerical columns.
- Investigate the distribution of categorical variables, such as "Country Code," "City," and "Cuisines."
- Identify the top cuisines and cities with the highest number of restaurants, gaining insights into customer preferences.

**Task 3: Geospatial Analysis**

- Visualize restaurant locations using latitude and longitude information, providing a spatial perspective.
- Analyze the geographical distribution of restaurants across different cities and countries.
- Explore potential correlations between the restaurant's location and its rating, uncovering location-based patterns.

# ***Let's Begin***

## ***Task 1: Data Exploration and Preprocessing***

### Import Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
# Importing Libraries
import pandas as pd
import numpy as np

# Visualization Libraries
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

# Ignore all warnings
import warnings

warnings.filterwarnings('ignore')

"""### Dataset Loading"""

# Load Dataset from github repository
df = pd.read_csv("https://raw.githubusercontent.com/Apaulgithub/Restaurant_Data_Analysis_and_Prediction/main/Dataset.csv")

"""### Dataset First View"""

# Dataset First Look
# View top 5 rows of the dataset
df.head()

"""### Dataset Rows & Columns count"""

# Dataset Rows & Columns count
# Checking number of rows and columns of the dataset using shape
print("Number of rows are: ",df.shape[0])
print("Number of columns are: ",df.shape[1])

"""### Duplicate Values"""

# Dataset Duplicate Value Count
dup = df.duplicated().sum()
print(f'number of duplicated rows are {dup}')

"""### Missing Values/Null Values"""

# Missing Values/Null Values Count
df.isnull().sum()

# Visualizing the missing values
# Checking Null Value by Plotting Heatmap

# Set the plot size
plt.figure(figsize = (10,6))

# Create the figure object
sns.heatmap(df.isnull().corr(), vmin=-1, annot= True)

# Set labels
plt.xlabel('Name of columns', fontsize=11)
plt.ylabel('Name of columns', fontsize=10)
plt.title('Places of missing values in Dataset', fontsize=12)

# To show
plt.show()

"""#### Handling Missing Values"""

# If the null values number will high, then we can replace it with any placeholder value. In case if we drop them, we will loss a lot of data
# So, since Cuisines column have low number of missing values, that is only 9, i have dropping the same
df = df.dropna(subset=['Cuisines'])

# Checking missing values again for confirmation
print("Missing values/null values count after handling:")
df.isna().sum()

"""### Data Type Conversion"""

# Dataset Information
# Checking information about the dataset using info
df.info()

"""Data type conversion is not needed here, everything is looking fine.

### Distribution of The Target Variable
"""

# Distribution of the target variable ("Aggregate rating") and identify class imbalance
target_counts = df['Aggregate rating'].value_counts()
print("Distribution of target variable:")
print(target_counts)

"""### What did i found from the level 1 (task 1)?

* The Restuarant dataset consists of various restuarants information of different cities. Includes information such as restaurant name, city, address, locality, cuisines, rating and price range, among other things.
* There are 9551 rows and 21 columns provided in the data.
* Null values are only present in cuisines; Since there are only few null values present in cuisines (only 9) i will remove them from the data.
* No duplicate values exist.
* Data type conversion not required.
* Distribution of the target variable ("Aggregate rating") well balanced.

## ***Task 2: Descriptive Analysis***

### Statistical Measures for Numerical Columns
"""

# Basic statistical measures (mean, median, standard deviation, etc.) for numerical columns
# Select Numerical Columns
numeric_columns = df.select_dtypes(include=['int', 'float'])

# Calculate basic statistical measures using .describe()
summary_stats = numeric_columns.describe()
print(summary_stats)

# Individual statistics
# Calculate mean for numerical columns
mean = numeric_columns.mean()
print(f"Mean for numerical columns:\n{mean}")

# Calculate median for numerical columns
median = numeric_columns.median()
print(f"\nMedian for numerical columns:\n{median}")

# Calculate standard deviation for numerical columns
std_dev = numeric_columns.std()
print(f"\nStandard deviation for numerical columns:\n{std_dev}")

"""### Distribution of Categorical Variables"""

# Distribution of categorical variables like 'Country Code', 'City', and 'Cuisines'

# Count Plot Visualization Code for Country Codes
# Set plot size
plt.figure(figsize=(8, 5))

# Create the figure object
sns.countplot(x = df['Country Code'])

# Set Labels
plt.xlabel('Country Codes')
plt.ylabel('Number of Restaurants')
plt.title('Distribution of Restaurants by Country Codes')

# Display Chart
plt.show()

# Count Plot Visualization Code for Cities
# Set plot size
plt.figure(figsize=(8, 5))

# Create the figure object
# There are many cities names present in the data, so i select only the top 10 cities
sns.countplot(y = df['City'], order=df.City.value_counts().iloc[:10].index)

# Set Labels
plt.xlabel('Number of Restaurants')
plt.ylabel('Name of Cities')
plt.title('Top 10 Cities with Highest Number of Restaurants')

# Display Chart
plt.show()

# Count Plot Visualization Code for Cuisines
# Set plot size
plt.figure(figsize=(8, 5))

# Create the figure object
# There are many cuisine names present in the data, so i select only the top 10 cuisines
sns.countplot(y = df['Cuisines'], order=df.Cuisines.value_counts().iloc[:10].index)

# Set Labels
plt.xlabel('Number of Restaurants')
plt.ylabel('Name of Cuisines')
plt.title('Top 10 Cuisines with Highest Number of Restaurants')

# Display Chart
plt.show()

"""### Top Cuisines and Cities"""

# Top cuisines and cities with the highest number of restaurants

# Identify the top 10 cuisines
top_cuisines = df['Cuisines'].value_counts().head(10)

# Display the results
print("Top 10 Cuisines with Highest Number of Restaurants:")
print(top_cuisines)

# Identify the top 10 cities
top_cities = df['City'].value_counts().head(10)

# Display the results
print("Top 10 Cities with Highest Number of Restaurants:")
print(top_cities)

"""### What did i found from the level 1 (task 2)?

* Found the mean, median, mode values and other statistical measures for the numerical columns like 'Restaurant ID', 'Longitude', 'Latitude', 'Price range', etc.
* Country code 1 and 216 are with highest number of restaurants.
* New Delhi, Gurgaon and Noida are in top with highest number of restaurants.
* North Indian and Chinese cuisine are in top with highest number of restaurants.

## ***Task 3: Geospatial Analysis***

### Visualize Locations of Restaurants
"""

# Locations of restaurants on a map using latitude and longitude information
# Import the necessary libraries
from shapely.geometry import Point
import geopandas as gpd
from geopandas import GeoDataFrame

# Create Point geometry from latitude and longitude using Shapely
gdf = gpd.GeoDataFrame(
    df,
    geometry=gpd.points_from_xy(df.Longitude, df.Latitude)
)

# Create a base map of the world using Geopandas
world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))

# Create a map that fits the screen and plots the restaurant locations
# The "continent" column is used for coloring and a legend is displayed
gdf.plot(ax=world.plot("continent", legend = True, figsize=(14, 12)), marker='o', color='red', markersize=15)

# Show the map
plt.show()

"""### Distribution of Restaurants by City"""

# Distribution of restaurants across different cities or countries
# Set plot size
plt.figure(figsize=(8, 5))

# Create the figure object
# There are many cities names present in the data, so i select only the top 10 cities
sns.countplot(y = df['City'], order=df.City.value_counts().iloc[:10].index)

# Set Labels
plt.xlabel('Number of Restaurants')
plt.ylabel('Name of Cities')
plt.title('Distribution of Restaurants Across Cities')

# Display Chart
plt.show()

"""### Correlation Between the Restaurant's Location and its Rating"""

# Checking correlation between the restaurant's location and its rating
# Set plot size
plt.figure(figsize=(10, 6))

# Calculate the correlation between latitude, longitude, and ratings
correlation_matrix = df[['Latitude', 'Longitude', 'Aggregate rating']].corr()

# Create a heatmap to visualize the correlation
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")

# Set Title
plt.title("Correlation Between Restaurant's location and Rating")

# Display Chart
plt.show()

"""### What did i found from the level 1 (task 3)?

* North America and Asia(mainly India) have the most number of restaurants. Followed by Oceania and others.
* New Delhi have the most number of restaurants. Followed by Gurgaon, Noida and Faridabad.
* There is no correlation between  Latitude and Rating. But, Longitude and Rating are negatively correlated.

# ***Conclusion***

The insights which i found from the overall level 1 project:

**Data Overview:**

- The dataset includes restaurant details across various cities with 9,551 rows and 21 columns.
- Minimal null values (9) were found only in the 'Cuisines' column.
- No duplicates exist, and data type conversion wasn't needed.
- The 'Aggregate rating' distribution is well-balanced.

**Descriptive Insights:**

- Key statistical measures for numerical columns were identified.
- Country codes 1 and 216 have the most restaurants.
- New Delhi, Gurgaon, and Noida are top cities with the highest restaurant counts.
- North Indian and Chinese cuisines are most popular.

**Geospatial Analysis:**

- North America and Asia (mainly India) have the most number of restaurants.
- New Delhi leads in the number of restaurants, followed by Gurgaon, Noida, and Faridabad.
- Latitude and rating show no correlation, while longitude and rating are negatively correlated.

These insights offer a comprehensive analysis of the restaurant dataset reveals key data characteristics, descriptive insights, and geospatial patterns, informs further analysis.
"""